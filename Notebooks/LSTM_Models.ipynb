{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "98dc1aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For notebooks â€” get the current working directory\n",
    "notebook_dir = os.getcwd()\n",
    "project_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "# Then import your module\n",
    "#import Utils.functions as data_viz\n",
    "import Utils.file_io as file_io\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9826407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, average_precision_score, balanced_accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Input, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.metrics import AUC, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bdef7687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded from /workspaces/RecessionPredictionML/Notebooks/Dataset/data_features_daily.csv\n",
      "DataFrame loaded from /workspaces/RecessionPredictionML/Notebooks/Dataset/data_features_weekly.csv\n",
      "DataFrame loaded from /workspaces/RecessionPredictionML/Notebooks/Dataset/data_features_monthly.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"{project_dir}/Notebooks/Dataset/data_features\"\n",
    "\n",
    "df_features_daily = file_io.input_csv(f\"{file_path}_daily\")\n",
    "df_features_weekly = file_io.input_csv(f\"{file_path}_weekly\")\n",
    "df_features_monthly = file_io.input_csv(f\"{file_path}_monthly\")\n",
    "\n",
    "#recessions = pd.read_csv(f\"{project_dir}/Dataset/recession_periods.csv\")\n",
    "#recessions = file_io.input_csv(f\"{project_dir}/Dataset/recession_periods\")\n",
    "recessions = pd.read_csv(f\"{project_dir}/Dataset/recession_periods.csv1\")\n",
    "\n",
    "dict_features = {'Daily': df_features_daily,\n",
    "                 'Weekly': df_features_weekly,\n",
    "                 'Monthly': df_features_monthly\n",
    "} \n",
    "\n",
    "train_test_split = pd.to_datetime('2015-01-01')\n",
    "split_at = train_test_split\n",
    "\n",
    "export_config = {'Print Out For all models': \n",
    "                  {'save': False},\n",
    "                 'Save Probability Plots as PNG': \n",
    "                  {'save': False},\n",
    "                 'Export AUC Report to CSV': \n",
    "                  {'save': False}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "29facf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor\n",
    "def binary_focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        eps = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, eps, 1. - eps)\n",
    "        \n",
    "        cross_entropy = - (y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "\n",
    "        loss = alpha_factor * modulating_factor * cross_entropy\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "def make_sequences(X, y=None, seq_len=12):\n",
    "    X_seq, y_seq, timestamps = [], [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        X_seq.append(X.iloc[i:i+seq_len].values)\n",
    "        timestamps.append(X.index[i+seq_len])\n",
    "        #print(timestamps)\n",
    "        if y is not None:\n",
    "            y_seq.append(y.iloc[i+seq_len])\n",
    "\n",
    "    X_seq = np.array(X_seq)\n",
    "    return (X_seq, np.array(y_seq)) if y is not None else (X_seq, None)\n",
    "\n",
    "\n",
    "def get_class_weight(y_train):\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    alpha = class_weights[1] / (class_weights[0] + class_weights[1])\n",
    "    return class_weights_dict, alpha\n",
    "    \n",
    "    \n",
    "def scaling_features(train,test, scaler_func = StandardScaler()):\n",
    "    scaler = scaler_func\n",
    "    X_train = train\n",
    "    X_test = test\n",
    "    n_train, seq_len, n_feat = X_train.shape\n",
    "    n_test,      _   ,  _    = X_test.shape\n",
    "    if scaler_func is None:\n",
    "        return X_train, X_test, seq_len, n_feat\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train.reshape(-1, n_feat)).reshape(n_train, seq_len, n_feat)\n",
    "    X_test_scaled  = scaler.transform    (X_test.reshape(-1, n_feat)).reshape(n_test, seq_len, n_feat)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, seq_len, n_feat\n",
    "\n",
    "\n",
    "\n",
    "def layer_sizes(str_var):\n",
    "    str_var = 'LSTM_4_2'\n",
    "    parts = str_var.split('_')\n",
    "    int_list = [int(x) for x in parts[1:]]\n",
    "    model_type = parts[0]\n",
    "    return int_list, model_type\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "339c40b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef scaling_features1(train, test, scaler_func=StandardScaler()):\\n    # Helper function to check and extract info & reshape if input is a DataFrame\\n    def prepare_data(X):\\n        if isinstance(X, pd.DataFrame) and isinstance(X.index, pd.MultiIndex):\\n            # Extract unique sequences (level 0) and sequence length (level 1)\\n            seq_ids = X.index.get_level_values(0).unique()\\n            seq_len = X.index.get_level_values(1).nunique()\\n            n_feat = X.shape[1]\\n\\n            # Convert DataFrame to numpy array of shape (n_seq, seq_len, n_feat)\\n            X_np = X.values.reshape(len(seq_ids), seq_len, n_feat)\\n            return X_np, seq_ids, seq_len, n_feat\\n        else:\\n            # Assume numpy array input\\n            n_seq, seq_len, n_feat = X.shape\\n            return X, None, seq_len, n_feat\\n\\n    X_train_np, train_seq_ids, seq_len, n_feat = prepare_data(train)\\n    X_test_np, test_seq_ids, _, _ = prepare_data(test)\\n\\n    if scaler_func is None:\\n        # Return input as is, but convert DataFrame to numpy if needed\\n        return X_train_np, X_test_np, seq_len, n_feat\\n\\n    scaler = scaler_func\\n    n_train = X_train_np.shape[0]\\n    n_test = X_test_np.shape[0]\\n\\n    # Flatten for scaler\\n    X_train_scaled = scaler.fit_transform(X_train_np.reshape(-1, n_feat)).reshape(n_train, seq_len, n_feat)\\n    X_test_scaled = scaler.transform(X_test_np.reshape(-1, n_feat)).reshape(n_test, seq_len, n_feat)\\n\\n    # If original input was DataFrame, reconstruct scaled DataFrames with same MultiIndex and columns\\n    if train_seq_ids is not None:\\n        # Rebuild MultiIndex for train\\n        train_index = pd.MultiIndex.from_product([train_seq_ids, range(seq_len)], names=[\\'sequence\\', \\'step\\'])\\n        X_train_scaled = pd.DataFrame(X_train_scaled.reshape(-1, n_feat), index=train_index, columns=train.columns)\\n\\n        # Rebuild MultiIndex for test\\n        test_index = pd.MultiIndex.from_product([test_seq_ids, range(seq_len)], names=[\\'sequence\\', \\'step\\'])\\n        X_test_scaled = pd.DataFrame(X_test_scaled.reshape(-1, n_feat), index=test_index, columns=test.columns)\\n\\n    return X_train_scaled, X_test_scaled, seq_len, n_feat\\n\\ndef make_sequences1(X: pd.DataFrame, y: pd.Series = None, seq_len: int = 12):\\n    X_seq_list, y_seq_list, timestamps = [], [], []\\n\\n    for i in range(len(X) - seq_len):\\n        seq_df = X.iloc[i:i+seq_len]\\n        X_seq_list.append(seq_df)\\n        timestamps.append(X.index[i + seq_len])\\n        if y is not None:\\n            y_seq_list.append(y.iloc[i + seq_len])\\n\\n    # Combine all sequences into a MultiIndex DataFrame\\n    X_seq = pd.concat(X_seq_list, keys=timestamps, names=[\"timestamp\", \"step\"])\\n\\n    if y is not None:\\n        y_seq = pd.Series(y_seq_list, index=timestamps, name=\\'target\\')\\n        return X_seq, y_seq\\n    else:\\n        return X_seq, None\\n'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def scaling_features1(train, test, scaler_func=StandardScaler()):\n",
    "    # Helper function to check and extract info & reshape if input is a DataFrame\n",
    "    def prepare_data(X):\n",
    "        if isinstance(X, pd.DataFrame) and isinstance(X.index, pd.MultiIndex):\n",
    "            # Extract unique sequences (level 0) and sequence length (level 1)\n",
    "            seq_ids = X.index.get_level_values(0).unique()\n",
    "            seq_len = X.index.get_level_values(1).nunique()\n",
    "            n_feat = X.shape[1]\n",
    "            \n",
    "            # Convert DataFrame to numpy array of shape (n_seq, seq_len, n_feat)\n",
    "            X_np = X.values.reshape(len(seq_ids), seq_len, n_feat)\n",
    "            return X_np, seq_ids, seq_len, n_feat\n",
    "        else:\n",
    "            # Assume numpy array input\n",
    "            n_seq, seq_len, n_feat = X.shape\n",
    "            return X, None, seq_len, n_feat\n",
    "    \n",
    "    X_train_np, train_seq_ids, seq_len, n_feat = prepare_data(train)\n",
    "    X_test_np, test_seq_ids, _, _ = prepare_data(test)\n",
    "\n",
    "    if scaler_func is None:\n",
    "        # Return input as is, but convert DataFrame to numpy if needed\n",
    "        return X_train_np, X_test_np, seq_len, n_feat\n",
    "\n",
    "    scaler = scaler_func\n",
    "    n_train = X_train_np.shape[0]\n",
    "    n_test = X_test_np.shape[0]\n",
    "\n",
    "    # Flatten for scaler\n",
    "    X_train_scaled = scaler.fit_transform(X_train_np.reshape(-1, n_feat)).reshape(n_train, seq_len, n_feat)\n",
    "    X_test_scaled = scaler.transform(X_test_np.reshape(-1, n_feat)).reshape(n_test, seq_len, n_feat)\n",
    "\n",
    "    # If original input was DataFrame, reconstruct scaled DataFrames with same MultiIndex and columns\n",
    "    if train_seq_ids is not None:\n",
    "        # Rebuild MultiIndex for train\n",
    "        train_index = pd.MultiIndex.from_product([train_seq_ids, range(seq_len)], names=['sequence', 'step'])\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled.reshape(-1, n_feat), index=train_index, columns=train.columns)\n",
    "\n",
    "        # Rebuild MultiIndex for test\n",
    "        test_index = pd.MultiIndex.from_product([test_seq_ids, range(seq_len)], names=['sequence', 'step'])\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled.reshape(-1, n_feat), index=test_index, columns=test.columns)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, seq_len, n_feat\n",
    "\n",
    "def make_sequences1(X: pd.DataFrame, y: pd.Series = None, seq_len: int = 12):\n",
    "    X_seq_list, y_seq_list, timestamps = [], [], []\n",
    "\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq_df = X.iloc[i:i+seq_len]\n",
    "        X_seq_list.append(seq_df)\n",
    "        timestamps.append(X.index[i + seq_len])\n",
    "        if y is not None:\n",
    "            y_seq_list.append(y.iloc[i + seq_len])\n",
    "\n",
    "    # Combine all sequences into a MultiIndex DataFrame\n",
    "    X_seq = pd.concat(X_seq_list, keys=timestamps, names=[\"timestamp\", \"step\"])\n",
    "    \n",
    "    if y is not None:\n",
    "        y_seq = pd.Series(y_seq_list, index=timestamps, name='target')\n",
    "        return X_seq, y_seq\n",
    "    else:\n",
    "        return X_seq, None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d5992b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#X_train_scaled, X_test_scaled, seq_len, n_feat = scaling_features(X_train, X_test) #one time calculation per time_freq?\n",
    "\n",
    "#class_weights_dict, alpha = compute_class_weight(y_train) # one time calculation per time_freq?\n",
    "\n",
    "def LSTM_Model_init(model_name, seq_len, n_feat, alpha):\n",
    "\n",
    "    layer_size, model_type = layer_sizes(model_name)\n",
    "\n",
    "    is_bi = model_type.upper().startswith(\"BILSTM\")\n",
    "\n",
    "    #Build model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(seq_len, n_feat)))\n",
    "    \n",
    "\n",
    "    for i, units in enumerate(layer_size):\n",
    "        return_seq = (i < len(layer_size) - 1)\n",
    "        lstm_layer = LSTM(units, return_sequences=return_seq)\n",
    "\n",
    "        if is_bi:\n",
    "            # wrap in Bidirectional\n",
    "            model.add(Bidirectional(lstm_layer))\n",
    "        else:\n",
    "            model.add(lstm_layer)\n",
    "        \n",
    "        model.add(Dropout(0.3))\n",
    "    # stuff\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=binary_focal_loss(gamma=2.0, alpha=alpha),\n",
    "        #metrics=[\"accuracy\"]\n",
    "        metrics=[#\"Precision\", \"Recall\", #\"AUC\"]\n",
    "                    AUC(name='AUC-ROC', curve = 'ROC'),\n",
    "                    AUC(name='AUC-PR', curve='PR')]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "#early_stop = EarlyStopping(monitor=\"val_Recall\", patience=10, restore_best_weights=True, mode='max')\n",
    "\n",
    "def LSTM_Model_train(train, test, early_stop, model, class_weights, verbose = 0):\n",
    "    X_train_scaled, y_train = train[0], train[1]\n",
    "    X_test_scaled, y_test = test[0], test[1]\n",
    "    callback_list = [early_stop] if early_stop is not None else []\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=50, batch_size=32,\n",
    "        callbacks=callback_list,\n",
    "        class_weight=class_weights,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    #X_all_scaled = np.concatenate([X_train_scaled, X_test_scaled], axis=0) #pd.concat([X_train_scaled, X_test_scaled])\n",
    "    #probs_all = model.predict(X_all_scaled, verbose=verbose).reshape(-1)\n",
    "    epochs = len(history.history['loss'])\n",
    "\n",
    "    probs = model.predict(X_test_scaled, verbose=verbose).reshape(-1)\n",
    "    probs_all = probs\n",
    "    return model, epochs, probs_all\n",
    "\n",
    "def LSTM_Model_evaluate(test, probs, threshold = None):\n",
    "    X_test_scaled, y_test = test[0], test[1]\n",
    "    if threshold is None:\n",
    "        threshold = 0.001\n",
    "    y_pred = (probs >= threshold).astype(int)\n",
    "\n",
    "    #precision, recall, thresholds = precision_recall_curve(y_test, probs)\n",
    "    #cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=[\"No Recession\", \"Recession\"], output_dict=True, zero_division=0)\n",
    "    #report[\"auc_pr\"] = auc(recall, precision)\n",
    "    report[\"ap_score\"]=  average_precision_score(y_test, probs)\n",
    "    report[\"auc_roc\"]= roc_auc_score(y_test, probs)\n",
    "\n",
    "    #epochs = len(history.history['loss'])\n",
    "    return report\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e7f04462",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_models = ['LSTM_4_2', 'LSTM_4_4']\n",
    "LSTM_results = {}\n",
    "early_stop = EarlyStopping(monitor=\"val_AUC-ROC\", patience=10, restore_best_weights=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "61eb18b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time_freq = \\'Monthly\\'\\nmodel_name = LSTM_models[0]\\ndf = dict_features[time_freq]\\nX = df.drop(columns=\\'recession\\')\\ny = df[\\'recession\\'] \\n\\n# Train-test split\\n#split_at = pd.to_datetime(\\'2015-01-01\\')\\nX_train, X_test = X[X.index < split_at], X[X.index >= split_at]\\ny_train, y_test = y[y.index < split_at].astype(int), y[y.index >= split_at].astype(int)\\n# Convert to sequences\\nseq_len = 32\\nX_train_seq, y_train_seq = make_sequences(X_train, y_train, seq_len)\\nX_test_seq, y_test_seq = make_sequences(X_test, y_test, seq_len)\\nX_train_scaled, X_test_scaled, _, n_feat = scaling_features(X_train_seq, X_test_seq) \\nclass_weights_dict, alpha = get_class_weight(y_train_seq) \\nLSTM_results[time_freq] = {}\\nLSTM_results[time_freq][model_name] = {}\\nprint(model_name)\\nmodel_init = LSTM_Model_init(model_name, seq_len, n_feat, alpha)\\nprint(\\'Done with Init\\')\\nmodel, epoch, probs = LSTM_Model_train(train = [X_train_scaled,y_train_seq], test = [X_test_scaled, y_test_seq], \\n                 model = model_init, early_stop = early_stop, class_weights = class_weights_dict, \\n                 verbose = 0)\\nprint(\\'Done with training\\')\\n\\nreport = LSTM_Model_evaluate(test = [_, y_test_seq], probs = probs, threshold = None)  \\n\\n#LSTM_results[time_freq][model_name][\\'probs\\'] = probs\\nLSTM_results[time_freq][model_name][\\'report\\'] = report\\nLSTM_results[time_freq][model_name][\\'epoch\\'] = epoch\\nLSTM_results[time_freq][model_name][\\'model\\'] = model\\n\\nprint(LSTM_results[time_freq][model_name][\\'report\\'])\\n\\nperiod = time_freq\\n\\n\\nprint(f\"\\n=== {time_freq} | {model_name} ===\")\\n#print(\"Confusion Matrix:\\n\", LSTM_results[time_freq][f\\'{model_name}_confusion_matrix\\'])\\nprint(\"Classification Report:\")\\nprint(pd.DataFrame(LSTM_results[time_freq][model_name][\\'report\\']).transpose())\\n'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''time_freq = 'Monthly'\n",
    "model_name = LSTM_models[0]\n",
    "df = dict_features[time_freq]\n",
    "X = df.drop(columns='recession')\n",
    "y = df['recession'] \n",
    "\n",
    "# Train-test split\n",
    "#split_at = pd.to_datetime('2015-01-01')\n",
    "X_train, X_test = X[X.index < split_at], X[X.index >= split_at]\n",
    "y_train, y_test = y[y.index < split_at].astype(int), y[y.index >= split_at].astype(int)\n",
    "# Convert to sequences\n",
    "seq_len = 32\n",
    "X_train_seq, y_train_seq = make_sequences(X_train, y_train, seq_len)\n",
    "X_test_seq, y_test_seq = make_sequences(X_test, y_test, seq_len)\n",
    "X_train_scaled, X_test_scaled, _, n_feat = scaling_features(X_train_seq, X_test_seq) \n",
    "class_weights_dict, alpha = get_class_weight(y_train_seq) \n",
    "LSTM_results[time_freq] = {}\n",
    "LSTM_results[time_freq][model_name] = {}\n",
    "print(model_name)\n",
    "model_init = LSTM_Model_init(model_name, seq_len, n_feat, alpha)\n",
    "print('Done with Init')\n",
    "model, epoch, probs = LSTM_Model_train(train = [X_train_scaled,y_train_seq], test = [X_test_scaled, y_test_seq], \n",
    "                 model = model_init, early_stop = early_stop, class_weights = class_weights_dict, \n",
    "                 verbose = 0)\n",
    "print('Done with training')\n",
    "\n",
    "report = LSTM_Model_evaluate(test = [_, y_test_seq], probs = probs, threshold = None)  \n",
    "        \n",
    "#LSTM_results[time_freq][model_name]['probs'] = probs\n",
    "LSTM_results[time_freq][model_name]['report'] = report\n",
    "LSTM_results[time_freq][model_name]['epoch'] = epoch\n",
    "LSTM_results[time_freq][model_name]['model'] = model\n",
    "\n",
    "print(LSTM_results[time_freq][model_name]['report'])\n",
    "\n",
    "period = time_freq\n",
    "\n",
    "\n",
    "print(f\"\\n=== {time_freq} | {model_name} ===\")\n",
    "#print(\"Confusion Matrix:\\n\", LSTM_results[time_freq][f'{model_name}_confusion_matrix'])\n",
    "print(\"Classification Report:\")\n",
    "print(pd.DataFrame(LSTM_results[time_freq][model_name]['report']).transpose())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f734ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dda9c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_4_2\n",
      "Done with Init\n",
      "Done with training LSTM_4_2 with 27 epochs\n",
      "LSTM_4_4\n",
      "Done with Init\n",
      "Done with training LSTM_4_4 with 17 epochs\n",
      "LSTM_4_2\n",
      "Done with Init\n",
      "Done with training LSTM_4_2 with 14 epochs\n",
      "LSTM_4_4\n",
      "Done with Init\n",
      "Done with training LSTM_4_4 with 30 epochs\n",
      "LSTM_4_2\n",
      "Done with Init\n",
      "Done with training LSTM_4_2 with 29 epochs\n",
      "LSTM_4_4\n",
      "Done with Init\n",
      "Done with training LSTM_4_4 with 29 epochs\n"
     ]
    }
   ],
   "source": [
    "for time_freq, df in dict_features.items():\n",
    "    X = df.drop(columns='recession')\n",
    "    y = df['recession'] \n",
    "\n",
    "    # Train-test split\n",
    "    #split_at = pd.to_datetime('2015-01-01')\n",
    "    X_train, X_test = X[X.index < split_at], X[X.index >= split_at]\n",
    "    y_train, y_test = y[y.index < split_at].astype(int), y[y.index >= split_at].astype(int)\n",
    "\n",
    "    # Convert to sequences\n",
    "    seq_len = 32\n",
    "    X_train_seq, y_train_seq = make_sequences(X_train, y_train, seq_len)\n",
    "    X_test_seq, y_test_seq = make_sequences(X_test, y_test, seq_len)\n",
    "\n",
    "    X_train_scaled, X_test_scaled, _, n_feat = scaling_features(X_train_seq, X_test_seq) #one time calculation per time_freq?\n",
    "    class_weights_dict, alpha = get_class_weight(y_train_seq) # one time calculation per time_freq?\n",
    "\n",
    "    LSTM_results[time_freq] = {}\n",
    "    for model_name in LSTM_models:\n",
    "        LSTM_results[time_freq][model_name] = {}\n",
    "        print(model_name)\n",
    "        model_init = LSTM_Model_init(model_name, seq_len, n_feat, alpha)\n",
    "        print('Done with Init')\n",
    "        model, epoch, probs = LSTM_Model_train(train = [X_train_scaled,y_train_seq], test = [X_test_scaled, y_test_seq], \n",
    "                         model = model_init, early_stop = early_stop, class_weights = class_weights_dict, \n",
    "                         verbose = 0)\n",
    "        print(f'Done with training {model_name} with {epoch} epochs')\n",
    "        report = LSTM_Model_evaluate(test = [_, y_test_seq], probs = probs, threshold = None)  \n",
    "        \n",
    "        #LSTM_results[time_freq][model_name]['probs'] = probs_all\n",
    "        LSTM_results[time_freq][model_name]['report'] = report\n",
    "        LSTM_results[time_freq][model_name]['epoch'] = epoch\n",
    "        LSTM_results[time_freq][model_name]['model'] = model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "acd348c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Daily | LSTM_4_2 ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score      support\n",
      "No Recession   0.000000  0.000000  0.000000  2439.000000\n",
      "Recession      0.115022  1.000000  0.206313   317.000000\n",
      "accuracy       0.115022  0.115022  0.115022     0.115022\n",
      "macro avg      0.057511  0.500000  0.103157  2756.000000\n",
      "weighted avg   0.013230  0.115022  0.023730  2756.000000\n",
      "ap_score       0.073456  0.073456  0.073456     0.073456\n",
      "auc_roc        0.247368  0.247368  0.247368     0.247368\n",
      "\n",
      "=== Daily | LSTM_4_4 ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score      support\n",
      "No Recession   0.000000  0.000000  0.000000  2439.000000\n",
      "Recession      0.115022  1.000000  0.206313   317.000000\n",
      "accuracy       0.115022  0.115022  0.115022     0.115022\n",
      "macro avg      0.057511  0.500000  0.103157  2756.000000\n",
      "weighted avg   0.013230  0.115022  0.023730  2756.000000\n",
      "ap_score       0.082562  0.082562  0.082562     0.082562\n",
      "auc_roc        0.328535  0.328535  0.328535     0.328535\n",
      "\n",
      "=== Weekly | LSTM_4_2 ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score     support\n",
      "No Recession   0.000000  0.000000  0.000000  457.000000\n",
      "Recession      0.117761  1.000000  0.210708   61.000000\n",
      "accuracy       0.117761  0.117761  0.117761    0.117761\n",
      "macro avg      0.058880  0.500000  0.105354  518.000000\n",
      "weighted avg   0.013868  0.117761  0.024813  518.000000\n",
      "ap_score       0.189729  0.189729  0.189729    0.189729\n",
      "auc_roc        0.743157  0.743157  0.743157    0.743157\n",
      "\n",
      "=== Weekly | LSTM_4_4 ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score     support\n",
      "No Recession   0.000000  0.000000  0.000000  457.000000\n",
      "Recession      0.117761  1.000000  0.210708   61.000000\n",
      "accuracy       0.117761  0.117761  0.117761    0.117761\n",
      "macro avg      0.058880  0.500000  0.105354  518.000000\n",
      "weighted avg   0.013868  0.117761  0.024813  518.000000\n",
      "ap_score       0.120291  0.120291  0.120291    0.120291\n",
      "auc_roc        0.549916  0.549916  0.549916    0.549916\n",
      "\n",
      "=== Monthly | LSTM_4_2 ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score    support\n",
      "No Recession   0.000000  0.000000  0.000000  81.000000\n",
      "Recession      0.147368  1.000000  0.256881  14.000000\n",
      "accuracy       0.147368  0.147368  0.147368   0.147368\n",
      "macro avg      0.073684  0.500000  0.128440  95.000000\n",
      "weighted avg   0.021717  0.147368  0.037856  95.000000\n",
      "ap_score       0.134923  0.134923  0.134923   0.134923\n",
      "auc_roc        0.455026  0.455026  0.455026   0.455026\n",
      "\n",
      "=== Monthly | LSTM_4_4 ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score    support\n",
      "No Recession   0.000000  0.000000  0.000000  81.000000\n",
      "Recession      0.147368  1.000000  0.256881  14.000000\n",
      "accuracy       0.147368  0.147368  0.147368   0.147368\n",
      "macro avg      0.073684  0.500000  0.128440  95.000000\n",
      "weighted avg   0.021717  0.147368  0.037856  95.000000\n",
      "ap_score       0.175374  0.175374  0.175374   0.175374\n",
      "auc_roc        0.604938  0.604938  0.604938   0.604938\n"
     ]
    }
   ],
   "source": [
    "if True:#export_config['Print Out For all models']['save']:\n",
    "    for time_freq in LSTM_results.keys():\n",
    "        for model_name in LSTM_results[time_freq].keys():\n",
    "            print(f\"\\n=== {time_freq} | {model_name} ===\")\n",
    "            #print(\"Confusion Matrix:\\n\", LSTM_results[time_freq][f'{model_name}_confusion_matrix'])\n",
    "            print(\"Classification Report:\")\n",
    "            print(pd.DataFrame(LSTM_results[time_freq][model_name]['report']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0aa6cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[192]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m var = LSTM_results[\u001b[33m'\u001b[39m\u001b[33mDaily\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mLSTM_4_4\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(var)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m var.keys():\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(key)\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "var = LSTM_results['Daily']['LSTM_4_4']['epoch']\n",
    "print(var)\n",
    "print(len(var))\n",
    "\n",
    "for key in var.keys():\n",
    "    print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
